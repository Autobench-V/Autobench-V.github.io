<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Evaluating mathematical reasoning of foundation models in visual contexts">
  <meta name="keywords" content="MathVista, Math Vista">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title> AutoBench-V: Can Large Vision-Language Models Benchmark Themselves? </title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
/Users/panlu/Library/Mobile Documents/com~apple~CloudDocs/ImageMath/visual-mathqa-server/data_final/images
    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link rel="icon" href="./static/images/logos/judge.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <!-- <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bulma.min.css" rel="stylesheet">
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script> -->
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>

  <script src="./static/js/leaderboard_testmini.js"></script>  
  <script src="./data/results/output_folders.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>

  <script src="./visualizer/data/data_public.js" defer></script>
    <style>
        table {
            width: 100%; /* Â∞ÜÊï¥‰ΩìÂÆΩÂ∫¶ÂáèÂ∞è */
            border-collapse: collapse;
            margin: auto; /* ËÆ©Ë°®Ê†ºÂ±Ö‰∏≠ÊòæÁ§∫ */
        }
        th, td {
            padding: 4px; /* ÂáèÂ∞èÂçïÂÖÉÊ†ºÁöÑÂÜÖËæπË∑ù */
            border: 1px solid #ddd;
            font-size: 0.9em; /* ÂèØÈÄâÔºöÂáèÂ∞èÂ≠ó‰ΩìÂ§ßÂ∞è‰ª•ËäÇÁúÅÁ©∫Èó¥ */
        }
        .highlight {
            background-color: #f0f0f0;
            color: #9932CC; /* Ëøë‰ºº‰∫éviolet */
        }
        thead {
            background-color: #f2f2f2;
        }
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
        .checkmark {
            color: green;
        }
        .xmark {
            color: red;
        }
        .xcheck {
            color: lightskyblue;
        }
        caption {
            caption-side: top;
            text-align: center;
            font-weight: bold;
            margin-bottom: 8px;
        }
.hero {
    position: relative;
    overflow: hidden; /* Á°Æ‰øùËÉåÊôØ‰∏çË∂ÖÂá∫Ëøô‰∏™Âå∫Âüü */
}

.hero .background {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background-image: url('static/images/background.jpg');
    background-size: cover;
    background-position: center;
    filter: blur(5px); /* È´òÊñØÊ®°Á≥äÊïàÊûú */
    z-index: -1; /* Á°Æ‰øùËÉåÊôØÂú®ÂÜÖÂÆπ‰πã‰∏ã */
}

.hero-body {
    position: relative;
    z-index: 1; /* Á°Æ‰øùÂÜÖÂÆπÊòæÁ§∫Âú®ËÉåÊôØ‰πã‰∏ä */
    /* ÂÖ∂‰ªñÊ†∑Âºè‰øùÊåÅ‰∏çÂèò */
}

        .video-container {
            max-width: 60%;
            margin: 0 auto;
            padding: 20px;
            background-color: #f9f9f9;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            border-radius: 10px;
            overflow: hidden;
        }
        .video-container iframe {
            width: 100%;
            height: 315px;
            border: none;
        }
        .video-title {
            text-align: center;
            font-size: 1.5em;
            margin-bottom: 15px;
            color: #333;
        }
    </style>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <!-- <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a> -->
      <!-- @PAN TODO: consider adding links? -->
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://gui-world.github.io/">
            <b>GUI-World</b> <p style="font-size:18px; display: inline; margin-left: 5px;">üî•</p>
          </a>
          <a class="navbar-item" href="https://github.com/Flossiee/HonestyLLM">
            <b>HonestyLLM</b> <p style="font-size:18px; display: inline; margin-left: 5px;">üî•</p>
          </a>
          <a class="navbar-item" href="https://trustllmbenchmark.github.io/TrustLLM-Website/">
            <b>TrustLLM</b> <p style="font-size:18px; display: inline; margin-left: 5px;">üî•</p>
          </a>
          <a class="navbar-item" href="https://mllm-judge.github.io">
            <b>MLLM-as-a-Judge</b> <p style="font-size:18px; display: inline; margin-left: 5px;">üî•</p>
          </a>
          <a class="navbar-item" href="https://llm-coauthor.github.io/">
            LLM-as-a-Coauthor
          </a>
          <a class="navbar-item" href="https://unigen-framework.github.io/">
            UniGen
          </a>
          <a class="navbar-item" href="https://llm-judge-bias.github.io/">
            LLM-judge-bias
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
    <!-- <div class="background"></div> Ê∑ªÂä†ÁöÑËÉåÊôØÂ±Ç -->
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <img src="static/images/logo.png" style="width:7em;vertical-align: middle" alt="Logo"/>
            </h1>
            <h2 class="subtitle is-3 publication-subtitle" style="color: rgb(255, 255, 255); font-weight: bold; text-shadow: 2px 2px 4px rgba(255,255,255, 0.5);">
    <span style="color: rgb(0,0, 0); font-weight: bold;">AutoBench-V: Can Large Vision-Language Models Benchmark Themselves?</span>
            </h2>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                  <a href="https://github.com/wad3birch">Han Bao</a><sup style="color:#3b69ab;">*,‚Ä†</sup>,
              </span>
              <span class="author-block">
                <a href="https://howiehwong.github.io/">Yue Huang</a><sup style="color:#6fbf73;">1, *</sup>,
              </span>
              <span class="author-block">
                  <a href="https://wyf23187.github.io/">Yanbo Wang</a><sup style="color:#3b69ab;">*,‚Ä†</sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/Y0oMu">Jiayi Ye</a><sup style="color:#3b69ab;">*,‚Ä†</sup>,
              </span>
              <span class="author-block">
                <a href="">Xiangqi Wang</a><sup style="color:#6fbf73;">1</sup>,
              </span>
              <span class="author-block">
                  <a href="https://iriscxy.github.io/">Xiuying Chen</a><sup style="color:#ed4b82;">2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.mohamed-elhoseiny.com/">Mohamed Elhoseiny</a><sup style="color:#1d9a29;">3</sup>,
              </span>
              <span class="author-block">
                  <a href="https://engineering.nd.edu/faculty/xiangliang-zhang/">Xiangliang Zhang</a><sup style="color:#6fbf73;">1,‚Ä†‚Ä†</sup>
              </span>
          </div>
          <div class="is-size-5 publication-authors">
              <span class="author-block"><sup style="color:#6fbf73;">1</sup>University of Notre Dame,</span>
              <span class="author-block"><sup style="color:#ed4b82;">2</sup>MBZUAI,</span>
              <span class="author-block"><sup style="color:#1d9a29;">3</sup>KAUST</span>
          </div>
          
          <div>
              (* Equal Contribution, ‚Ä† Independent Researcher, ‚Ä†‚Ä† Corresponding Author)
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <!-- @PAN TODO: change links -->
                <a href="#" class="external-link button is-normal is-rounded is-dark"><!-- TODO -->
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark"><!-- TODO -->
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="#" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Demo</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Autobench-V/Autobench-V.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">  
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code & Data</span>
                  </a>
              </span>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<br>

<br>
<br>
<br>
<br>
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="content has-text-centered">
      <img src="static/images/tease_scores_gpt4v.png" alt="geometric reasoning" width="99%"/>
      <p> Accuracy scores of one leading LLM (i.e., PoT GPT-4), four primary LMMs, random chance, and human performance our proposed 
      <img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
      <span class="mathvista">MathVista</span>
      across mathematical reasoning and visual context types. PoT refers to program-of-thought prompting, and PoT GPT-4 is a textual LLM augmented with the caption and OCR text. GPT-4V is manually evaluated via the playground chatbot.
      </p>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container" style="margin-top: -150px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/autobench-v.png" alt="geometric reasoning" width="84%"/>
              <p>A Comprehensive Overview of the <b>AutoBench-V </b>Framework
              </p>
            </div>
          </div>
             <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/pie.png" alt="geometric reasoning" width="84%"/>
              <p>
                Five key evaluation dimensions  supported by <b>AutoBench-V</b>, along with their fine-grained sub-aspects, accompanied by questions and images  to assist in understanding. 
            </p>
            </div>
          </div>
        </div>
      </div>
    </div>
</section>

<!-- <div class="video-container">
        <div class="video-title">How to Use UniGen?</div>
        <iframe src="https://www.youtube.com/embed/kWVC7GGGh2o" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </div> -->


    <section class="section">
      <div class="container" style="margin-bottom: 2vh;">
        <!-- Introduction. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Large Vision-Language Models (LVLMs) have become essential for advancing the integration of visual and linguistic information, facilitating a wide range of complex applications and tasks. However, the evaluation of LVLMs presents significant challenges as the evaluation benchmark always demands lots of human cost for its construction, and  remains static, lacking flexibility once constructed. Even though automatic evaluation has been explored in textual modality, the visual modality remains under-explored. As a result, in this work, we address a question: "Can LVLMs serve as a path to automatic benchmarking?". We introduce <strong>AutoBench-V</strong>, an automated framework for serving evaluation on demand, i.e., benchmarking LVLMs based on specific aspects of model capability. Upon receiving an evaluation capability, <strong>AutoBench-V</strong> leverages text-to-image models to generate relevant image samples and then utilizes LVLMs to orchestrate visual question-answering (VQA) tasks, completing the evaluation process efficiently and flexibly. Through an extensive evaluation of seven popular LVLMs across five demanded user inputs (i.e., evaluation capabilities), the framework shows effectiveness and reliability. We observe the following: (1) Our constructed benchmark accurately reflects varying task difficulties; (2) As task difficulty rises, the performance gap between models widens; (3) While models exhibit strong performance in abstract level understanding, they underperform in details reasoning tasks; and (4) Constructing a dataset with varying levels of difficulties is critical for a comprehensive and exhaustive evaluation. Overall, <strong>AutoBench-V</strong> not only successfully utilizes LVLMs for automated benchmarking but also reveals that LVLMs as judges have significant potential in various domains.
              </p>
            </div>
            <h2 class="title is-3">Introduction</h2>
            <div class="content has-text-justified">
            <p>
                The flourishing of Large Language Models (LLMs) has paved the way for significant advancements in the field of natural language processing (NLP). As the capabilities of LLMs grew, researchers began to explore the integration of visual information understanding capabilities into LLMs, giving rise to the development of Large Vision-Language models (LVLMs). These models are trained on extensive paired image-text datasets, enabling them to perform sophisticated multimodal reasoning by effectively integrating visual and textual information.
            </p>
            
            <p>
                With the widespread adoption of LVLMs, evaluating these models has become increasingly important for understanding their limitations and reliability better. Recent research emphasizes the urgent need for comprehensive and sophisticated evaluation standards that accurately assess LVLMs' abilities across various modalities. Various benchmarks aim to evaluate a range of capabilities of LVLMs including 3D understanding, perception and cognition capacity, and multi-discipline understanding and reasoning. Even though these works have solidly evaluated certain aspects of LVLMs' capabilities, they lack the flexibility to support on-demand evaluation across various capability aspects. Recent studies have explored the usage of generative AI in automating evaluation, which offers flexibility in varying evaluation dimensions and reduces the human cost of benchmark dataset construction. While these studies focus on the automatic evaluation of LLMs, we aim to extend this to visual modality.
            </p>
            
            <p>
                Automating the evaluation of LVLMs presents several key challenges. First, the targeted capabilities to be evaluated must be clearly identified based on the input demand. This is the foundation that relevant images and appropriate visual question-answering (VQA) tasks can be generated to accurately assess the LVLMs' performance in those specific aspects. Second, the generated images and VQA tasks should be relevant and accurately reflect the evaluation target. Third, the risk of answer leakage from the Examiner LVLM during question generation should be mitigated.
            </p>
            
            <p>
                To address the above challenges, we propose AutoBench-V, which supports automated evaluation of LVLMs based on a user demand regarding specific aspects of model capability (e.g., Spatial Understanding). Initially, the input demand is processed by an examiner LVLM, which categorizes it into several overarching aspects. Each aspect is further divided into several fine-grained components, for which image descriptions of varying difficulty levels are generated. To ensure that the descriptions align with their corresponding images, a self-validation mechanism is applied using VQA. Furthermore, an error control mechanism is implemented to prevent a negative impact on the generation of questions and reference answers. The generated questions and images are then presented to the evaluated LVLM to generate responses, which are assessed against reference answers.
            </p>       
          </div>
    
            <div>
                <h2 class="title is-3">Contributions</h2>
                <div class="content has-text-justified">
                    <ul>
                        <li><strong>An automated LVLM evaluation framework. </strong> This proposed AutoBench-V is the first automated framework for benchmarking LVLMs' capability. The framework leverages text-to-image models to generate images for evaluation and employs GPT-4o as an examiner to conduct VQA evaluations. This automation significantly reduces human involvement, enhancing the efficiency and objectivity of the evaluation process.</li>
                        <li><strong>Extensive experiments validating the framework's effectiveness.</strong>  We conducted comprehensive experiments, including main evaluations on multiple models, examiner superiority tests, option position bias analysis, and human assessments. The results confirm the framework‚Äôs robustness and effectiveness in evaluating LVLMs.</li>
                        <li><strong>In-depth analysis of LVLMs' performance across diverse visual tasks.</strong> Through systematic evaluation with varied user inputs, we find that LVLMs demonstrate strong proficiency in abstract conceptual understanding while exhibiting comparatively lower performance in concrete visual reasoning tasks. These insights offer a perspective on the current state of LVLM technology, highlighting areas with potential for future development and exploration.</li>
                    </ul>
                </div>
            </div>
        </div>
        <!--/ Introduction. -->
      </div>
    </section>

<!-- <section class="section">
  
    <div class="container" style="margin-bottom: 2vh;">
        <div class="columns is-centered has-text-centered">
    <table>
        <caption>Comparison of different dataset generation frameworks. The lightblue checkmark means the work may achieve parts of the goal (not all).</caption>
        <thead>
            <tr>
                <th>Work</th>
                <th>Generali.</th>
                <th>Control.</th>
                <th>Diversity</th>
                <th>Truthful</th>
                <th>w/o Human.</th>
                <th>New Knowledge</th>
                <th>Dynamic Bench.</th>
                <th>Data Aug.</th>
            </tr>
        </thead>
        <tbody>
            <tr style="background-color: #e0e0e0;">
                <td>DyVal (2024)</td>
                <td class="xmark">‚úó</td>
                <td class="xmark">‚úó</td>
                <td class="xmark">‚úó</td>
                <td class="checkmark">‚úî</td>
                <td class="checkmark">‚úî</td>
                <td class="xmark">‚úó</td>
                <td class="checkmark">‚úî</td>
                <td class="checkmark">‚úî</td>
            </tr>
            <tr>
                <td>DyVal 2 (2024)</td>
                <td class="checkmark">‚úî</td>
                <td class="xcheck">‚úî</td>
                <td class="xmark">‚úó</td>
                <td class="xmark">‚úó</td>
                <td class="checkmark">‚úî</td>
                <td class="xcheck">‚úî</td>
                <td class="checkmark">‚úî</td>
                <td class="checkmark">‚úî</td>
            </tr>
            <tr style="background-color: #e0e0e0;">
                <td>S3Eval (2024)</td>
                <td class="xmark">‚úó</td>
                <td class="checkmark">‚úî</td>
                <td class="xmark">‚úó</td>
                <td class="xmark">‚úó</td>
                <td class="checkmark">‚úî</td>
                <td class="xmark">‚úó</td>
                <td class="checkmark">‚úî</td>
                <td class="xmark">‚úó</td>
            </tr>
                        <tr>
                <td>Yu et al. (2024)</td>
                <td class="xcheck">‚úî</td>
                <td class="checkmark">‚úî</td>
                <td class="checkmark">‚úî</td>
                <td class="xmark">‚úó</td>
                <td class="checkmark">‚úî</td>
                <td class="checkmark">‚úî</td>
                <td class="xmark">‚úó</td>
                <td class="checkmark">‚úî</td>
            </tr>
            <tr style="background-color: #e0e0e0;">
                <td>Chung et al. (2023)</td>
                <td class="xmark">‚úó</td>
                <td class="xmark">‚úó</td>
                <td class="checkmark">‚úî</td>
                <td class="checkmark">‚úî</td>
                <td class="xmark">‚úó</td>
                <td class="xmark">‚úó</td>
                <td class="xmark">‚úó</td>
                <td class="xmark">‚úó</td>
            </tr>
            <tr>
                <td>Fan et al. (2024)</td>
                <td class="xmark">‚úó</td>
                <td class="xmark">‚úó</td>
                <td class="xmark">‚úó</td>
                <td class="checkmark">‚úî</td>
                <td class="checkmark">‚úî</td>
                <td class="xmark">‚úó</td>
                <td class="checkmark">‚úî</td>
                <td class="xmark">‚úó</td>
            </tr>
            <tr style="background-color: #e0e0e0;">
                <td>Jandaghi et al. (2023)</td>
                <td class="xmark">‚úó</td>
                <td class="xmark">‚úó</td>
                <td class="xmark">‚úó</td>
                <td class="xmark">‚úó</td>
                <td class="checkmark">‚úî</td>
                <td class="checkmark">‚úî</td>
                <td class="xmark">‚úó</td>
                <td class="xmark">‚úó</td>
            </tr>
            <tr>
                <td>Wang et al. (2024)</td>
                <td class="checkmark">‚úî</td>
                <td class="xmark">‚úó</td>
                <td class="xmark">‚úó</td>
                <td class="xcheck">‚úî</td>
                <td class="checkmark">‚úî</td>
                <td class="xcheck">‚úî</td>
                <td class="checkmark">‚úî</td>
                <td class="xmark">‚úó</td>
            </tr>
            <tr style="background-color: #e0e0e0;">
                <td><strong>UniGen</strong></td>
                <td class="checkmark">‚úî</td>
                <td class="checkmark">‚úî</td>
                <td class="checkmark">‚úî</td>
                <td class="checkmark">‚úî</td>
                <td class="checkmark">‚úî</td>
                <td class="checkmark">‚úî</td>
                <td class="checkmark">‚úî</td>
                <td class="checkmark">‚úî</td>
            </tr>

        </tbody>
    </table>
</div>
        </div>
</section> -->
<!-- <section class="section">
    <div class="container" style="margin-bottom: 2vh;">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
          <h2 class="title is-3">Bias Type, Description and Example</h2>
          <br>
          <table style="font-size: 1em; width: 100%; border-collapse: collapse;">
            <caption style="caption-side: top; text-align: left; margin-bottom: 10px;">
              Types of biases in LLM-as-a-Judge, with descriptions and examples that demonstrate how particular bias affects LLM's judgment. For instance, in the Self-Enhancement bias, we illustrate how a model that is both the evaluator and the evaluated (Assistant A) might show a preference for its answers, even when anonymized.
            </caption>
            <thead>
              <tr style="background-color: #f2f2f2;">
                <th style="padding: 10px; text-align: center; border-bottom: 2px solid #ddd;">Bias Type</th>
                <th style="padding: 10px; text-align: center; border-bottom: 2px solid #ddd;">Description</th>
                <th style="padding: 10px; text-align: center; border-bottom: 2px solid #ddd;">Example</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;"><strong>üîÄ Position (Pos.)</strong></td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">When an LLM exhibits a propensity to favor certain positions over others.</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">
                  <span style="color: #15be75;">Assistant A: 3.11>3.8</span> 
                  <span style="color: #877eeb;">Assistant B: 3.8>3.11</span><br>
                  <span style="color: #877eeb;">Assistant B: 3.8>3.11</span> 
                  <span style="color: #15be75;">Assistant A: 3.11>3.8</span>
                </td>
              </tr>
              <tr style="background-color: #f9f9f9;">
                <td style="padding: 10px; border-bottom: 1px solid #ddd;"><strong>üìÑ Verbosity (Ver.)</strong></td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">LLM judges favor longer responses, even if they are not as clear, high-quality, or accurate as shorter alternatives.</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">
                  <span style="color: #15be75;">Assistant A: As we all know, in mathematics, 3.11 is greater than 3.8</span> <i>(Longer)</i><br>
                  <span style="color: #877eeb;">Assistant B: 3.11>3.8</span>
                </td>
              </tr>
              <tr>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;"><strong>üï¥ Self-Enhancement (Sel.)</strong></td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">LLM judges may favor the answers generated by themselves.</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">
                  <span style="color: #15be75;">Assistant A: 3.11>3.8</span> (<i>Judge Model</i>)<br>
                  <span style="color: #877eeb;">Assistant B: 3.8>3.11</span>
                </td>
              </tr>
              <tr style="background-color: #f9f9f9;">
                <td style="padding: 10px; border-bottom: 1px solid #ddd;"><strong>üé≠ Compassion-Fade (Com.)</strong></td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">The tendency to observe different behaviors when given recognizable names as opposed to anonymized aliases.</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">
                  <span style="color: #15be75;">GPT-4: 3.11>3.8</span><br>
                  <span style="color: #877eeb;">Llama-7B: 3.8>3.11</span><br>
                  (There is <i>NO ANONYMITY</i> in being Assistant)
                </td>
              </tr>
              <tr>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;"><strong>üë• Bandwagon (Ban.)</strong></td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">The tendency to give stronger preference to the majority's beliefs.</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">
                  <span style="color: #15be75;">Assistant A: 3.11>3.8</span><br>
                  (System: <i>90%</i> believe that Assistant A is better.)
                </td>
              </tr>
              <tr style="background-color: #f9f9f9;">
                <td style="padding: 10px; border-bottom: 1px solid #ddd;"><strong>ü•ù Distraction (Dis.)</strong></td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">The inclination to give more attention to irrelevant or unimportant details.</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">
                  <span style="color: #15be75;">Assistant A: 3.11>3.8</span><br>
                  (System: Assistant A loves eating pasta, especially pasta with homemade tomato sauce.)
                </td>
              </tr>
              <tr>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;"><strong>üëÅ Fallacy-Oversight (Fal.)</strong></td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">LLM judges may ignore logical problems in reasoning and give wrong judgment.</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">
                  <span style="color: #15be75;">Assistant A: We compare the decimal part first, <i>because 0.11>0.8, so 3.11>3.8</i></span>
                </td>
              </tr>
              <tr style="background-color: #f9f9f9;">
                <td style="padding: 10px; border-bottom: 1px solid #ddd;"><strong>‚úç Authority (Aut.)</strong></td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">Models may tend to assign more credibility to statements made by authority figures, regardless of actual evidence.</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">
                  <span style="color: #15be75;">Assistant A: 3.11>3.8(Citation: Patel, R. (2018). Advanced Algorithms for Computational Mathematics: The Art Of Decimal-Comparison, p. 143)</span>
                </td>
              </tr>
              <tr>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;"><strong>üòÇ Sentiment (Sen.)</strong></td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">Model have a preference for expressions of positive or negative emotions, affecting its judgment of emotional content.</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">
                  <span style="color: #15be75;">Assistant A: Regrettably, 3.11>3.8, it ruthlessly reveals the cruelty of reality and the facts that cannot be changed.</span> (<i>Frustrated tone</i>)
                </td>
              </tr>
              <tr style="background-color: #f9f9f9;">
                <td style="padding: 10px; border-bottom: 1px solid #ddd;"><strong>üéì Chain-of-Thought (CoT)</strong></td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">The model's evaluation results may vary with and without CoT.</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">
                  System: ...You should independently solve the user question <i>STEP-BY-STEP</i> first. Then compare both assistants' answers with your answer.
                </td>
              </tr>
              <tr>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;"><strong>üñã Refinement-Aware (Ref.)</strong></td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">Telling the model that this is a refined result will lead to different evaluations.</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">
                  <span style="color: #15be75;">Assistant A:"3.11>3.8" -> "3.11 is greater than 3.8"</span> (<i>The model REFINED his answer</i>)
                </td>
              </tr>
              <tr style="background-color: #f9f9f9;">
                <td style="padding: 10px; border-bottom: 1px solid #ddd;"><strong>‚öß Diversity (Div.)</strong></td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">Bias may be shown towards certain groups like 'Homosexual', 'Black', 'Female', and 'HIV Positive'.</td>
                <td style="padding: 10px; border-bottom: 1px solid #ddd;">
                  <span style="color: #15be75;">Assistant A: 3.11>3.8</span><br>
                  (System: Assistant A's true identity is <i>Homosexual</i>)
                </td>
              </tr>
            </tbody>
          </table>
        </div>
    </div>
        </div>
</section>
<section class="section">
  <div class="container">
      <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
              <h2 class="title is-3">Consistency Scores Overview</h2>
              <br>
              <table>
                  <caption>Consistency scores for various models across different metrics are presented. Dataset<sub>FR</sub> and Dataset<sub>AL</sub> represent Fact-Related datasets and Alignment datasets, respectively, while Rand<sub>FR</sub> and Rand<sub>AL</sub> indicate the consistency on these two datasets without changing any values.</caption>
                  <thead>
                      <tr>
                          <th rowspan="2" style="vertical-align: middle;">Model</th>
                          <th colspan="4">Dataset<sub>FR</sub> Consistency<span style="color: #8B0000;">‚Üë</span></th>
                          <th colspan="8">Dataset<sub>AL</sub> Consistency<span style="color: #8B0000;">‚Üë</span></th>
                      </tr>
                      <tr>
                          <th>Ver.</th>
                          <th>Fal.</th>
                          <th>Sen.</th>
                          <th>Rand<sub>FR</sub></th>
                          <th>Pos.</th>
                          <th>Com.</th>
                          <th>Ban.</th>
                          <th>Aut.</th>
                          <th>CoT.</th>
                          <th>Dst.</th>
                          <th>Div.</th>
                          <th>Rand<sub>Al</sub></th>
                      </tr>
                  </thead>
                  <tbody>
                      <tr>
                          <td>ChatGPT</td>
                          <td>0.900</td>
                          <td>0.917</td>
                          <td><strong>0.804</strong></td>
                          <td>0.998</td>
                          <td>0.566</td>
                          <td>0.862</td>
                          <td>0.688</td>
                          <td>0.662</td>
                          <td>0.699</td>
                          <td>0.713</td>
                          <td>0.679</td>
                          <td>0.906</td>
                      </tr>
                      <tr>
                          <td>GPT-4-Turbo</td>
                          <td>0.915</td>
                          <td>0.969</td>
                          <td>0.653</td>
                          <td>0.990</td>
                          <td>0.818</td>
                          <td>0.858</td>
                          <td>0.638</td>
                          <td>0.846</td>
                          <td>0.805</td>
                          <td>0.729</td>
                          <td>0.855</td>
                          <td>0.856</td>
                      </tr>
                      <tr>
                          <td>GPT-4o</td>
                          <td><strong>0.977</strong></td>
                          <td>0.984</td>
                          <td>0.699</td>
                          <td>0.998</td>
                          <td>0.776</td>
                          <td>0.868</td>
                          <td><strong>0.791</strong></td>
                          <td>0.787</td>
                          <td>0.833</td>
                          <td>0.790</td>
                          <td>0.814</td>
                          <td>0.925</td>
                      </tr>
                      <tr>
                          <td>GLM-4</td>
                          <td>0.887</td>
                          <td>0.979</td>
                          <td>0.679</td>
                          <td>0.970</td>
                          <td>0.781</td>
                          <td>0.835</td>
                          <td>0.690</td>
                          <td>0.796</td>
                          <td>0.814</td>
                          <td>0.814</td>
                          <td>0.788</td>
                          <td>0.884</td>
                      </tr>
                      <tr>
                          <td>Claude-3.5</td>
                          <td>0.952</td>
                          <td><strong>0.985</strong></td>
                          <td>0.660</td>
                          <td>0.999</td>
                          <td><strong>0.832</strong></td>
                          <td>0.875</td>
                          <td>0.610</td>
                          <td><strong>0.865</strong></td>
                          <td>0.857</td>
                          <td><strong>0.878</strong></td>
                          <td><strong>0.914</strong></td>
                          <td>0.915</td>
                      </tr>
                      <tr>
                          <td>Qwen2</td>
                          <td>0.884</td>
                          <td>0.935</td>
                          <td>0.651</td>
                          <td>0.994</td>
                          <td>0.760</td>
                          <td><strong>0.877</strong></td>
                          <td>0.710</td>
                          <td>0.779</td>
                          <td><strong>0.858</strong></td>
                          <td>0.785</td>
                          <td>0.826</td>
                          <td>0.904</td>
                      </tr>
                  </tbody>
              </table>
              <br>
              <br>
              <img src="static/images/consistency_all.png" alt="consistency_all" width="85%">
          </div>
      </div>
  </div>
</section> -->

<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Performance (Accuracy) details of all models on five user inputs and three difficulty levels</h2>
        <br>
        <table>
          <caption>Performance (Accuracy) details of all models on five user inputs and three difficulty levels</caption>
          <thead>
            <tr>
              <th rowspan="2" style="vertical-align: middle;">Model</th>
              <th colspan="6">User Input<span style="color: #f90101;">‚Üë</span></th>
            </tr>
            <tr>
              <th>Basic.</th>
              <th>Spatial.</th>
              <th>Seman.</th>
              <th>Reason.</th>
              <th>Atmos.</th>
              <th>Average</th>
            </tr>
          </thead>
          <tbody>
            <tr style="background-color: #e7e9ff;">
              <td colspan="7" class="text-bold">Easy</td>
            </tr>
            <tr>
              <td>GPT-4o</td>
              <td>90.18%</td>
              <td><strong>86.09%</strong></td>
              <td><strong>93.81%</strong></td>
              <td>88.13%</td>
              <td>93.94%</td>
              <td><strong>90.43%</strong></td>
            </tr>
            <tr>
              <td>GPT-4o mini</td>
              <td>90.18%</td>
              <td>81.28%</td>
              <td>91.24%</td>
              <td>81.92%</td>
              <td>95.45%</td>
              <td>88.01%</td>
            </tr>
            <tr>
              <td>Gemini-1.5-Flash</td>
              <td>89.29%</td>
              <td>81.82%</td>
              <td>91.19%</td>
              <td>85.31%</td>
              <td>92.75%</td>
              <td>88.07%</td>
            </tr>
            <tr>
              <td>Claude-3.5-Sonnet</td>
              <td>91.07%</td>
              <td>83.96%</td>
              <td>91.75%</td>
              <td>85.31%</td>
              <td>94.33%</td>
              <td>89.28%</td>
            </tr>
            <tr>
              <td>Claude-3-Haiku</td>
              <td>89.29%</td>
              <td>80.21%</td>
              <td>90.72%</td>
              <td>82.49%</td>
              <td>91.41%</td>
              <td>86.82%</td>
            </tr>
            <tr>
              <td>GLM-4v</td>
              <td><strong>91.96%</strong></td>
              <td>83.96%</td>
              <td>87.01%</td>
              <td><strong>92.78%</strong></td>
              <td><strong>96.45%</strong></td>
              <td><strong>90.43%</strong></td>
            </tr>
            <tr>
              <td>Qwen2-VL</td>
              <td>90.18%</td>
              <td>81.82%</td>
              <td>92.27%</td>
              <td>87.57%</td>
              <td>96.00%</td>
              <td>89.57%</td>
            </tr>
            <tr style="background-color: #e7e9ff;">
              <td colspan="7" class="text-bold">Medium</td>
            </tr>
            <tr>
              <td>GPT-4o</td>
              <td>76.87%</td>
              <td>72.25%</td>
              <td><strong>83.64%</strong></td>
              <td><strong>81.95%</strong></td>
              <td>84.35%</td>
              <td><strong>79.81%</strong></td>
            </tr>
            <tr>
              <td>GPT-4o mini</td>
              <td>76.19%</td>
              <td>67.26%</td>
              <td>79.09%</td>
              <td>77.56%</td>
              <td>84.78%</td>
              <td>76.98%</td>
            </tr>
            <tr>
              <td>Gemini-1.5-Flash</td>
              <td>73.47%</td>
              <td>66.96%</td>
              <td>78.18%</td>
              <td>70.44%</td>
              <td>84.14%</td>
              <td>74.64%</td>
            </tr>
            <tr>
              <td>Claude-3.5-Sonnet</td>
              <td>75.51%</td>
              <td>67.84%</td>
              <td>77.73%</td>
              <td>74.63%</td>
              <td>81.74%</td>
              <td>75.49%</td>
            </tr>
            <tr>
              <td>Claude-3-Haiku</td>
              <td>72.11%</td>
              <td>64.76%</td>
              <td>78.90%</td>
              <td>67.31%</td>
              <td>83.91%</td>
              <td>73.40%</td>
            </tr>
            <tr>
              <td>GLM-4v</td>
              <td>74.83%</td>
              <td><strong>74.89%</strong></td>
              <td>79.91%</td>
              <td>74.63%</td>
              <td>82.17%</td>
              <td>77.29%</td>
            </tr>
            <tr>
              <td>Qwen2-VL</td>
              <td><strong>82.31%</strong></td>
              <td>74.01%</td>
              <td>80.45%</td>
              <td>73.17%</td>
              <td><strong>85.21%</strong></td>
              <td>79.03%</td>
            </tr>
            <tr style="background-color: #e7e9ff;">
              <td colspan="7" class="text-bold">Hard</td>
            </tr>
            <tr>
              <td>GPT-4o</td>
              <td><strong>69.12%</strong></td>
              <td><strong>68.28%</strong></td>
              <td>79.36%</td>
              <td><strong>76.50%</strong></td>
              <td>81.82%</td>
              <td><strong>75.02%</strong></td>
            </tr>
            <tr>
              <td>GPT-4o mini</td>
              <td>63.43%</td>
              <td>61.23%</td>
              <td><strong>81.65%</strong></td>
              <td>69.94%</td>
              <td>77.27%</td>
              <td>70.70%</td>
            </tr>
            <tr>
              <td>Gemini-1.5-Flash</td>
              <td>66.91%</td>
              <td>65.20%</td>
              <td>77.10%</td>
              <td>68.16%</td>
              <td>76.88%</td>
              <td>70.85%</td>
            </tr>
            <tr>
              <td>Claude-3.5-Sonnet</td>
              <td>59.56%</td>
              <td>64.60%</td>
              <td>66.97%</td>
              <td>58.47%</td>
              <td>68.75%</td>
              <td>63.67%</td>
            </tr>
            <tr>
              <td>Claude-3-Haiku</td>
              <td>59.56%</td>
              <td>58.15%</td>
              <td>73.39%</td>
              <td>71.58%</td>
              <td>74.43%</td>
              <td>67.42%</td>
            </tr>
            <tr>
              <td>GLM-4v</td>
              <td>61.03%</td>
              <td>60.79%</td>
              <td>66.82%</td>
              <td>66.12%</td>
              <td>69.89%</td>
              <td>64.93%</td>
            </tr>
            <tr>
              <td>Qwen2-VL</td>
              <td>64.71%</td>
              <td>64.76%</td>
              <td>79.36%</td>
              <td>71.03%</td>
              <td><strong>79.59%</strong></td>
              <td>71.89%</td>
            </tr>
          </tbody>
        </table>
        <br>
        <br>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section">
  <div class="container">
      <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
              <h2 class="title is-3">Comparison of all models Consistency (score) and Robustness (error)</h2>
              <br>
              <div style="display: grid; grid-template-columns: repeat(2, 400px); justify-content: center; gap: 20px;">
                <figure>
                  <img src="static/images/leida1.png" alt="Image 1" style="width: 400px; height: 400px;">
                  <figcaption>Consistency (score)</figcaption>
                </figure>
                <figure>
                <img src="static/images/leida2.png" alt="Image 2" style="width: 400px; height: 400px;">
                <figcaption>Robustness (error)</figcaption>
                </figure>
              </div>
              <br>
          </div>
      </div>
  </div>
</section> -->


<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Figures</h2>
        <br>

        <div style="border: 2px solid #4CAF50; padding: 20px; margin: 20px; font-family: Arial, sans-serif; text-align: center;">
          <h2 style="color: #4CAF50;">Figure: Model performace</h2>
          <img src="static/images/Figure_rank.png" alt="figure rank" width="70%" />
          <p>Score variation of models from easy to hard difficulty across different user inputs. As task difficulty increases, the performance disparity between models becomes more pronounced.</p>
        </div>
        <br>

        <div style="border: 2px solid #337AB7; padding: 20px; margin: 20px; font-family: Arial, sans-serif; text-align: center;">
          <h2 style="color: #337AB7;">Figure: Samples</h2>
          <img src="static/images/figure_75sample.png" alt="75samples" width="70%" />
          <p>We randomly sampled 75 question instances for each user input, with 25 questions from each difficulty level. We then visualized the response patterns of various models to these questions (the red means). The rows in each user input, from top to bottom, represent the response situations of \texttt{Claude-3.5-sonnet}, \texttt{Claude-3-haiku}, \texttt{Gemini-1.5-flash}, \texttt{GLM-4v}, \texttt{GPT-4o}, \texttt{GPT-4o mini}, and \texttt{Qwen2-VL}, respectively.</p>
        </div>
        <br>

        <div style="border: 2px solid #b44672; padding: 20px; margin: 20px; font-family: Arial, sans-serif; text-align: center;">
          <h2 style="color: #b44672;">Figure: ranking</h2>
          <img src="static/images/heatmap_00.png" alt="heatmap" width="70%" />
          <p>The ranking of different models in five user inputs under different difficulty levels.</p>
        </div>
        <br>

        <div style="border: 2px solid #FF6347; padding: 20px; margin: 20px; font-family: Arial, sans-serif; text-align: center;">
          <h2 style="color: #FF6347;">Figure: examples</h2>
          <img src="static/images/picture_00.png" alt="picture samples" width="70%" />
          <p>Images examples corresponding to different user inputs under different difficulty levels.</p>
        </div>

      </div>
    </div>
  </div>
</section>




<!-- @PAN TODO: bibtex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>TODO</code></pre>
  </div>
</section>

<section>
  <div class="section" id="org-banners" style="display:flex">
    <a href="https://www.nd.edu/" target="blank" class="ext-link">
      <img class="center-block org-banner" src="static/images/logos/ND.png" style="width: 200px; height: auto;">
    </a>
    <a href="https://mbzuai.ac.ae/" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/logos/MBZUAI.png" style="width: 400px; height: auto;">
    </a>
    <a href="https://www.kaust.edu.sa/en/" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/logos/KAUST.webp" style="width: 300px; height: auto;">
    </a>
  </div>
</section>


<footer class="footer">
  <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  <!-- </div> -->
</footer>

</body>
</html>
